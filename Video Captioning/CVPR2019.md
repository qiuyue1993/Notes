# Video Captioning in CVPR 2019

## Indexing:
- [Streamlined Dense Video Captioning (Oral)](#Streamlined-Dense-Video-Captioning)
- [Memory-Attended Recurrent Network for Video Captioning](#Memory-Attended-Recurrent-Network-for-Video-Captioning)
- [Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning](#Object-Aware-Aggregation-With-Bidirectional-Temporal-Graph-for-Video-Captioning)
- [Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning](#Spatial-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning)
- [References](#References)

---
## Streamlined Dense Video Captioning
### Abstract
#### Task
- dense video captioning

#### Existing Approaches
*Processes*
- Firstly, detecting event proposals from a video
- Then, captioning on a subset of the proposals

*Problems of Existing Approaches*
- Generated sentences are prone to be **redundant**
- Generated sentences are prone to be **inconsistent considering the temporal dependency** between events

#### Proposed methods
*First*
- Models temporal dependency across event in a video explicitly (**RNN Structures?**)
- This is accomplished by integrating an event **sequence generation network to select a sequence of event proposals adaptively**

*Second*
- Levarages **visual and linguistic context from prior event** for coherent storytelling
- This is accompolished by feeding the sequence of event proposals to sequential video captioning network, which is trained by **reinforcement learning** with two-level rewards, at both **event and episode** levels.

#### Comparison between existing approaches and proposed approach

- Comparison

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Comparison.png" width="600" hegiht="400" align=center/>

### Framework

#### Overall Framework

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Overall-framework.png" width="600" hegiht="400" align=center/>

#### Event Proposal Network
- Input: video
- Output: a set of candidate event proposals

#### Event Sequence Generation Network
- detects an event sequence by selecting one out of the candidate event proposals

#### Sequential Captioning Network
- Input: detected event sequence
- Output: Generates captions sequentially, which is conditioned on **preceding events**

#### Training
- Three models are trained in a supervised manner
- The Sequential Captioning Network is optimized with reinforcement learning with two-level rewards

### Dataset, Evaluation Metrics, and Results
#### Dataset
*ActivityNet Captions*
- 20k YouTube videos
- avg. length of 120 seconds
- 10,024, 4,926, 5,044 for training, validation, test
- avg. 3.65 events
- avg. length of descriptions: 13.48 words

#### Evaluation Metrics
- performance evaluation tool 
- recall and precision of event proposal detection
- METEOR, CIDEr and BLEU for dense video captioning

#### Results
*Existing SOTA methods*
- DCE
- DVC
- Masked Transformer
- TDA-CG

*METEOR*
- 8.19

*Recall*
- 55.58 on average

*Precision*
- 57.57 on average

### Thoughts
- Event Sequence Generation Network
- **Why they train the Sequential Captioning Network with RL?**
---
## Memory-Attended Recurrent Network for Video Captioning
### Abstract
#### Conventional Video Captioning
- Encoder-decoder framework
- Focus on **only one source video**

*Disadvantage*
- Cannot capture the **multiple visual context information** of a word appearing in more than one relevant videos 

#### Proposed Video Captioning
- Memory-Attended Recurrent Network (MARN)
- **Memory structure**
- **Explore the full-spectrum correspondence** between a word and its various similiar visual contexts **across** videos in **training data**
- Enable **modeling the compatibility between adjacent words explicitly**

#### Illustration

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_MARN_Comparison.png" width="600" hegiht="400" align=center/>

### Framework
#### Overall Framework

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_MARN_Overall-framework.png" width="600" hegiht="400" align=center/>

#### Components
- Encoder: extracting features from source video (both 2D and 3D)
- Decoder: Attention-based Recurrent Decoder for captioning
- Decoder: Attended Memory Decoder (enhance the captioning quality)

### Dataset, Evaluation Metrics, and Results
#### Datasets
*Microsoft Research-Video to Text (MSR-VTT)*
- 10,000 video clips
- 20 general categories
- 20 human annotated natural sentences collected via AMT per video
- 6,513, 497, 2,990 for training, test, test

*Microsoft Research Video Description Corpus (MSVD)*
- 1,970 short video clips 
- Collected from YouTube
- A single activity for every video
- 40 captions / video
- 1,200, 100, 670 for training, validation, test

#### Evaluation Metrics
- CIDEr
- METEROR
- ROUGE-L
- BLEU

#### Results
- BLEU-4: 40.4
- METEROR: 28.1
- ROUGE-L: 60.7
- CIDEr: 47.1

### Thoughts
- Exciting Idea

---
## Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning
### Abstract
#### Intuition
- It is important for video captioning to capture **salient objects** with their **detailed temporal dynamics**
- And represent them using **discriminative spatio-temporal representations**

#### Proposed approach
- OA-BTG: object-aware aggregation with bidirectional temporal graph
- OA-BTG learns representations by performing **object-aware local feature aggregation** on **detected object regions**

#### Main novelties and advantages
*Bidirectional temporal graph*
- A bidirectional temporal graph is constructed **along and reversely along** the temporal graph
- Which provides **complementary ways** to capture the temporal trajectories for each salient object

*Object-aware aggregation*
- Learnable VLAD (**Vector of Locally Aggregated Descriptors**) models are constructed on **object temporal trajectories** and **global frame sequence**
- A **hierachical attention mechanism** is also developed to **distinguish different contributions of multiple objects**

### Framework
#### Overall Framework

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_OA-BTG_Overall-framework.png" width="600" hegiht="400" align=center/>

#### Bidirectional Temporal Graph
- Input: video
- First, extract frames and multiple object regions
- Next, construct bidirectional temporal graph 

#### Object-aware Aggregation
- Perform object-aware aggregation to **aggregate the local features of object regions and global frames** into discriminative **VLAD representations** using learnable VLAD models

#### Decoder
- Input: learned object and frame VLAD representation
- Using GRU unites to generate descriptions
- Hierachical attention is applied

### Dataset, Evaluation Metrics, and Results
#### Datasets
- MSVD
- MSR-VTT

#### Evaluation Metrics
- BLEU@4
- METEOR
- CIDEr

#### Results
*MSVD*
- BLEU@4: 56.9
- METEOR: 36.2
- CIDEr: 90.6


### Thoughts
- Focus on the spatio-temporal representations of salient objects
- Temporal and spatial attention for object features
- Temporal attention for global frames features 

--- 
## Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning
### Abstract
**



### Framework


### Dataset, Evaluation Metrics, and Results
#### Datasets
- MSVD
- MSR-VTT

#### Evaluation Metrics
- BLEU
- METEOR
- CIDEr
- ROUGE-L

#### Results
*MSVD*
- METEOR: 33.7

### Thoughts
- **Aggregating** the information as much as possible ?! (This paper aggregates the information from "object detection", "2D frames", "video clips")
- As the differences between different methods are small, methods with highly **interpretability** are acceptable? 

--- 
## References
- [Streamlined Dense Video Captioning](https://arxiv.org/pdf/1904.03870.pdf) 
- [Memory-Attended Recurrent Network for Video Captioning](https://arxiv.org/pdf/1905.03966.pdf)
- [Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning](https://arxiv.org/pdf/1906.04375.pdf)
- [Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning](https://arxiv.org/pdf/1902.10322.pdf)
---
