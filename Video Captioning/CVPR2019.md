# Video Captioning in CVPR 2019

## Indexing:
- [Streamlined Dense Video Captioning (Oral)](#Streamlined-Dense-Video-Captioning)
- [Memory-Attended Recurrent Network for Video Captioning](#Memory-Attended-Recurrent Network-for-Video-Captioning)
- [References](#References)

---
## Streamlined Dense Video Captioning
### Abstract
#### Task
- dense video captioning

#### Existing Approaches
*Processes*
- Firstly, detecting event proposals from a video
- Then, captioning on a subset of the proposals

*Problems of Existing Approaches*
- Generated sentences are prone to be **redundant**
- Generated sentences are prone to be **inconsistent considering the temporal dependency** between events

#### Proposed methods
*First*
- Models temporal dependency across event in a video explicitly (**RNN Structures?**)
- This is accomplished by integrating an event **sequence generation network to select a sequence of event proposals adaptively**

*Second*
- Levarages **visual and linguistic context from prior event** for coherent storytelling
- This is accompolished by feeding the sequence of event proposals to sequential video captioning network, which is trained by **reinforcement learning** with two-level rewards, at both **event and episode** levels.

#### Comparison between existing approaches and proposed approach

- Comparison

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Comparison.png" width="600" hegiht="400" align=center/>

### Framework

#### Overall Framework

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Overall-framework.png" width="600" hegiht="400" align=center/>

#### Event Proposal Network
- Input: video
- Output: a set of candidate event proposals

#### Event Sequence Generation Network
- detects an event sequence by selecting one out of the candidate event proposals

#### Sequential Captioning Network
- Input: detected event sequence
- Output: Generates captions sequentially, which is conditioned on **preceding events**

#### Training
- Three models are trained in a supervised manner
- The Sequential Captioning Network is optimized with reinforcement learning with two-level rewards

### Dataset, Evaluation Metrics, and Results
#### Dataset
*ActivityNet Captions*
- 20k YouTube videos
- avg. length of 120 seconds
- 10,024, 4,926, 5,044 for training, validation, test
- avg. 3.65 events
- avg. length of descriptions: 13.48 words

#### Evaluation Metrics
- performance evaluation tool 
- recall and precision of event proposal detection
- METEOR, CIDEr and BLEU for dense video captioning

#### Results
*Existing SOTA methods*
- DCE
- DVC
- Masked Transformer
- TDA-CG

*METEOR*
- 8.19

*Recall*
- 55.58 on average

*Precision*
- 57.57 on average

### Thoughts
- Event Sequence Generation Network
- **Why they train the Sequential Captioning Network with RL?**
---
## Memory-Attended Recurrent Network for Video Captioning




--- 
## References
- [Streamlined Dense Video Captioning](https://arxiv.org/pdf/1904.03870.pdf) 
- [Memory-Attended Recurrent Network for Video Captioning](https://arxiv.org/pdf/1905.03966.pdf)
---
