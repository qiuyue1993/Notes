# Video Captioning in CVPR 2019

## Indexing:
- [Streamlined Dense Video Captioning (Oral)](#Streamlined-Dense-Video-Captioning)
- [Memory-Attended Recurrent Network for Video Captioning](#Memory-Attended-Recurrent-Network-for-Video-Captioning)
- [Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning](#Object-Aware-Aggregation-With-Bidirectional-Temporal-Graph-for-Video-Captioning)
- [Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning](#Spatial-Temporal-Dynamics-and-Semantic-Attribute-Enriched-Visual-Encoding-for-Video-Captioning)
- [References](#References)

---
## Streamlined Dense Video Captioning
### Abstract
#### Task
- dense video captioning

#### Existing Approaches
*Processes*
- Firstly, detecting event proposals from a video
- Then, captioning on a subset of the proposals

*Problems of Existing Approaches*
- Generated sentences are prone to be **redundant**
- Generated sentences are prone to be **inconsistent considering the temporal dependency** between events

#### Proposed methods
*First*
- Models temporal dependency across event in a video explicitly (**RNN Structures?**)
- This is accomplished by integrating an event **sequence generation network to select a sequence of event proposals adaptively**

*Second*
- Levarages **visual and linguistic context from prior event** for coherent storytelling
- This is accompolished by feeding the sequence of event proposals to sequential video captioning network, which is trained by **reinforcement learning** with two-level rewards, at both **event and episode** levels.

#### Comparison between existing approaches and proposed approach

- Comparison

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Comparison.png" width="600" hegiht="400" align=center/>

### Framework

#### Overall Framework

<img src="https://github.com/qiuyue1993/Notes/blob/master/Video%20Captioning/Images/Paper-Summarize_SDVC_Overall-framework.png" width="600" hegiht="400" align=center/>

#### Event Proposal Network
- Input: video
- Output: a set of candidate event proposals

#### Event Sequence Generation Network
- detects an event sequence by selecting one out of the candidate event proposals

#### Sequential Captioning Network
- Input: detected event sequence
- Output: Generates captions sequentially, which is conditioned on **preceding events**

#### Training
- Three models are trained in a supervised manner
- The Sequential Captioning Network is optimized with reinforcement learning with two-level rewards

### Dataset, Evaluation Metrics, and Results
#### Dataset
*ActivityNet Captions*
- 20k YouTube videos
- avg. length of 120 seconds
- 10,024, 4,926, 5,044 for training, validation, test
- avg. 3.65 events
- avg. length of descriptions: 13.48 words

#### Evaluation Metrics
- performance evaluation tool 
- recall and precision of event proposal detection
- METEOR, CIDEr and BLEU for dense video captioning

#### Results
*Existing SOTA methods*
- DCE
- DVC
- Masked Transformer
- TDA-CG

*METEOR*
- 8.19

*Recall*
- 55.58 on average

*Precision*
- 57.57 on average

### Thoughts
- Event Sequence Generation Network
- **Why they train the Sequential Captioning Network with RL?**
---
## Memory-Attended Recurrent Network for Video Captioning
### Abstract
#### Conventional Video Captioning
- Encoder-decoder framework
- Focus on **only one source video**

*Disadvantage*
- Cannot capture the **multiple visual context information** of a word appearing in more than one relevant videos 

#### Proposed Video Captioning
- Memory-Attended Recurrent Network (MARN)
- **Memory structure**
- **Explore the full-spectrum correspondence** between a word and its various similiar visual contexts **across** videos in **training data**
- Enable **modeling the compatibility between adjacent words explicitly**

#### Illustration

- 

### Framework
#### Overall Framework

#### Components
- Encoder: extracting features from source video (both 2D and 3D)
- Decoder: Attention-based Recurrent Decoder for captioning
- Decoder: Attended Memory Decoder (enhance the captioning quality)

### Dataset, Evaluation Metrics, and Results
#### Datasets
*Microsoft Research-Video to Text (MSR-VTT)*
- 10,000 video clips
- 20 general categories
- 20 human annotated natural sentences collected via AMT per video
- 6,513, 497, 2,990 for training, test, test

*Microsoft Research Video Description Corpus (MSVD)*
- 1,970 short video clips 
- Collected from YouTube
- A single activity for every video
- 40 captions / video
- 1,200, 100, 670 for training, validation, test

#### Evaluation Metrics
- CIDEr
- METEROR
- ROUGE-L
- BLEU

#### Results
- BLEU-4: 40.4
- METEROR: 28.1
- ROUGE-L: 60.7
- CIDEr: 47.1

### Thoughts
- Exciting Idea

---
## Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning
### Abstract
**



### Framework


### Dataset, Evaluation Metrics, and Results


### Thoughts

--- 
## Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning
### Abstract
**



### Framework


### Dataset, Evaluation Metrics, and Results


### Thoughts

--- 
## References
- [Streamlined Dense Video Captioning](https://arxiv.org/pdf/1904.03870.pdf) 
- [Memory-Attended Recurrent Network for Video Captioning](https://arxiv.org/pdf/1905.03966.pdf)
- [Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning](https://arxiv.org/pdf/1906.04375.pdf)
- [Spatial-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning](https://arxiv.org/pdf/1902.10322.pdf)
---
