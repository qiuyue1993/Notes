## Vision and Language Papers of CVPR 2019

### Visual Question Answering

#### MUREL: Multimodal Relational Reasoning for Visual Question Answering (**6.18**; Poster)
- [Paper Link](https://arxiv.org/pdf/1902.09487.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/MUREL-Multimodal-Relational-Reasoning-for-Visual-Question-Answering.md)

- Sub-field: **Visual Question Answering**; **VQA2.0**

- Abstract: 

- Opinion: 


#### Towards VQA Models that can Read (**6.19**; Poster)
- [Paper Link](https://arxiv.org/pdf/1904.08920.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/Towards%20VQA%20Models%20That%20Can%20Read.md)

- Sub-field: **Visual Question Answering**; **Text-VQA**

- Abstract: Proposed a new VQA dataset that requiring answering a question about a text in a picture; Combines a OCR text recognition methods with conventional VQA in an integrated framework
 
- Opinion: Very good proposal of new VQA dataset;  Practical dataset for blind people

---
### Vision-Language Navigation

#### Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model (**6.18**; **Oral**)

- [Paper Link](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.pdf)

- [Summarization]()

- Abstract:

- Opinion:


#### Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation (**6.19**;**Oral**)
- [Paper Link](https://arxiv.org/pdf/1811.10092.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/Reinforced-Cross-Modal-Matching-and-Self-Supervised-Imitation-Learning-for-Vision-Language-Navigation.md)

- Sub-field: **Vision-Language Navigation**; **Reinforcement Learning**; **Imitation Learning**; **Matter**

- Abstract:

- Opinion: 



#### Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation (**6.19**; **Oral**)
- [Paper Link](https://arxiv.org/pdf/1903.02547.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/Tactical-Rewind-Self-Correction-via-Backtracking-in-Vision-and-Language-Navigation.md)

- Sub-field: **Vision-Language Navigation**; 

- Abstract: Conventional Vision-Language Navigation failed with a circulation and got lost in some occations; This work proposed a methods that can deal with this situation; Agent can balance the explore and exploit well to prevent from bad circling

- Opinion: Focus on a small point of this task


#### Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention (**6.20**; Poster)
- [Paper Link](https://arxiv.org/pdf/1812.04155.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/Vision-based%20Navigation%20with%20Language-based%20Assistance%20via%20Imitation%20Learning%20with%20Indirect%20Intervention.md)

- Sub-field: **Vision-Language Navigation**; **Imitation Learning**

- Abstract: Proposed a new vision-language navigation task that agent can request for language help in test time; Proposed a new imitation method that supports intervention.

- Opinion: Practicle in real-environment; Closer to real robot-human interaction.

#### The Regretful Agent: Heuristic-Aided Navigation Through Progress Estimation (**6.19**; **Oral**)
- [Paper Link](https://arxiv.org/pdf/1903.01602.pdf)

- [Summarization]()

- Astract:

- Opinion:

#### TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments (**6.20**; Poster)

- [Paper Link](https://nips2018vigil.github.io/static/papers/accepted/11.pdf)

- [Summarization]()

- Abstract:

- Opinion:

---
### Visual Dialog

#### Image-Question-Answer Synergistic Network for Visual Dialog (**6.20**; Poster)
- [Paper Link](https://arxiv.org/pdf/1902.09774.pdf)

- [Summarization](https://github.com/qiuyue1993/Notes/blob/master/CVPR_2019/Vision_and_Language/Paper_Summarize/Image-Question-Answer-Synergistic-Network-for-Visual-Dialog.md)

- Sub-field: **Visual Dialog**; **Visdial1.0**

- Abstract: Current VQA methods usually model the relationship of image and question, despite the information contains in the answers. Thus, these methods failed to predict answer with a lot of words. This work propose to use the information contain in the answer and model the relationship between image, question and answer. This work also proposed a two stage process, in the first stage, filter out the answers, in the next stage, rerank those answers.

- Opinionï¼šModeling the Image-Question-Answer relationship is important.

---
### Embodied Question Answering

#### Multi-Target Embodied Question Answering (**6.19**; Poster)

- [Paper Link](https://arxiv.org/pdf/1904.04686.pdf)

- [Summarization]()

- Abstract:

- Opinion:


#### Embodied Question Answering in Photorealistic Environments with Point Cloud Perception (**6.19**; **Oral**)

- [Paper Link](https://arxiv.org/pdf/1904.03461.pdf)

- [Summarization]()

- Abstract:

- Opinion:


###

---
### Visual Question Generation

#### What's to Know? Uncertainty as a Guide to Asking Goal-Oriented Questions (**6.18**; Poster)

- [Paper Link](https://arxiv.org/pdf/1812.06401.pdf)

- [Summarization]()

- Abstract:

- Opinion:




