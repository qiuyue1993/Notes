# 0617: Visual Question Answering and Dialog Workshop

## Indexing:
- [Opening Devi Pavikh](#Opening-Devi-Pavikh)
- [Invited Talk1 Alex Schwing](#Invited-Talk1-Alex-Schwing)
- [Invited Talk2 Lisa Hendricks](#Invited-Talk2-Lisa-Hendricks)
- [VQA Challenge](#VQA-Challenge)
- [Invited Talk3 Christopher Manning](#Invited-Talk3-Christopher-Manning)
- [GQA Challenge](#GQA-Challenge)
- [TextVQA Challenge](#TextVQA-Challenge)
- [Invited Talk4 Karl Moritz Hermann](#Invited-Talk4-Karl-Moritz-Hermann)
- [Invited Talk5 Layla El Asri](#Invited-Talk5-Layla-El-Asri)
- [Visual Dialog Challenge](#Visual-Dialog-Challenge)
- [Invited Talk6 Sanja Fidler](#Invited-Talk6-Sanja-Fidler)
- [Invited Talk7 Yoav Artzi](#Invited-Talk7-Yoav-Artzi)
- [Future Direction](#Future-Direction)
- [Closing Remark](#Closing-Remark)

---
## Opening Devi Pavikh
- 4 CHALLENGES: vqa2.0; textVQA; GQA; VisDial
- Agenda Announcement
 
---
## Invited Talk1 Alex Schwing
- Learning to Anticipate

### 
- models which analyze work very well
- We want to models which anticipate
- obsevation
- revealing priors
- seeing the unseen
- anticipate the future

# how
- 2d to 3d reasoning
- anticipate the 

### Factual VQA (NIPS2018)
- step1: fact retrieval
- step2: answer prediction

### Diversity
- How to generate diverse questions/descriptions
- How can we control diversity
- How can we build a mechanism to anticipate


### Classical Techniues for Image Captioning
- retrieval based (limited scope)
- extract image feature, extract high-score caption, via beamsearch (cost)

### Intuition: VAE (cvpr2017 spotlight)
- extract image feature; encode; decode (VAE based)
- generated diversity questions

- How to make this controllable
- how to get more diversity

### AG-CVAE (NIPS2017)
- controllability with AG-CVAE (NIPS 2017)
- resulted on diversity

### Convolutional Encoder and Decoder (CVPR2018)
- More flexible; 
- results in higher accuracy and higher entropy


### Controllability via part of speech tags (CVPR2019 Oral)
- Why?: encourage POS; result in a lot of words
- Results on Diversity

### Seq-CVAE Schematic 
- Results on higher diversity

- Diversity should be Fast; controllable; accurate
---
## Invited Talk2 Lisa Hendricks
- Diagnosing and Overcoming Bias in Captioning Models

### Bias in Image Captioning
- Captioning models looks at the wrong evidence
- Bias Amplication

### Women also Snowboard
- Wrong for the wrong reason
- Right for the wrong reason
- Equalizer Loss: Introducing Confident Loss and Appearance Confusion Loss along with the cross-entropy loss

*Dataset*
- MSCOCO; Biased; Balanced
- Proposed work is better
- Accounting for human uncertainty
- Right for the right reasons
- Lower bias amplification

### Object
- CHAIR Metric: determine hallucinated objects with both segmentation and caption annotations
- FIrst input feature have lower hallucination (instead of all stesp)
- Models with attention have lower hallucination
- self critical training (SC) leads to higher hallucination 
- Adversarial loss decreases hulluciation

*What causes hallucination*
- image or language model

*Dose standard metric capture hallucination*
- CHAIR is good
- traditional merics cannot

---
## VQA Challenge
### Task Overview
- What is VQA: requires a lot of abilities
- Datasets; Accuracy Metrics;

- challenge:41 teams
- runer -up msm at msra (75.01%)
- winnder (75.26%)


- STATISTICAL SIGNIFICANT
- +2.85% VS LAST YEAR
- percentage of correctly answered question grows
- Reasoning questions are more complicated

*Are models sensitive to subtle changes in images*
- +4.8%

*Are models driven by priors*
- 5-6% drops (questions have answers not in the first)

*Are models compositional*
- +3.57%
- - 11%

*Comparing with Human Accuracy*
- vqa2.0 human performance 80.78% vs 2019winner 75.26%

### Runner-up Talk
- Microsoft Asia

- Learning rich image region representation for visual question answering

#### baseline
- coattention
- glove+gru to encode the question


#### main tech
- learning rich image region features (adapted faster rcnn trained on visual genome1.2 dataset)
  - resnetxt-101 with fpn as 
  - region feature with attribute classification
  - stronger backbone
  - increase fpn dim
  - multi-scale training (used for detection performance)
  
- Powerful qauestion representation
 - replace glove and gru with bert
 - ensemble

### Winner Talk
- MCAN: Deep Modular Co-Attention Networks for Visual Question Answering
- inter- intra model attention modeling/dense interactions/dense reasoning

#### MCAN
- simultaneously model the desne intra- and inter- modal interaction
- enable visual reasoning

#### TIPS TRICKS
- larger model: hidden dimension
- better image feature: multi-view bottom-up attention
 - 5d bbox feature
 - grid feature
- bert
- model ensemble: 27 models
---- 
## Invited Talk3 Christopher Manning
- Making the L in VQA Matter

- VQA: Language of thought

- Faster rcnn, more non-vision people do vqa

Answers
- yes/no 38%
- comon numbers; 11%
- nearly one word stuff

Answers Not just one word
Answers should be interesting

Answers: Anything to challenge
- should we do real natural language generation
- visual dialog challenge
- maybe choosing from keyword answers is okays

Questions:
- shor, no complications


simple questions about obj and attri fail to test compositional language ability

Questions: The easy direction to expand

Another good direction? NLVR2
- GOALS ARE SIMILIAR

*wHAT'S STILL MISSING*
- deep understanding, coming from being able to reason in an abstract space

Abstraction: Towards a language of thought
- we see and reason with concepts, not visual details, 99% "scene gists"

Visual Genome
- region discreptions
- scene graph (objects + attributes + Relationships)

Content-based attention over concepts:
- attention over concept space,not over pixel space

GQA
- Visual Genome
- translate both modalities, both image and question to speak the same language
- image, scene graph, treat it as a **neural state machine**
- VQA-CP: VQA under Changing Priors (testing disentanglement)

To summarize:
- Iteractive attention over abstracted, disentangled concepts

---
## GQA Challenge
### Task Overview
*Overview*
- Exsiting Datasets: too short, not relational, not compositional, question biases, inconsistent
- GQA: Image replated with a scene graph; 100k images based on cleaned Visual Genome
- GQA Skills; spatial, comparative, logical, relational, compositional

*Analysis*
- semantics
- steps
- structure
- dataset balancing

*New Metrics*
- consistency
- validity
- plausibility
- grounding

*Winner*
- 52 teams
- Human:89.30 s MAC 54.06

- emsembling is very useful
- default object-based fetures
- question-structured representation

- binary obtain higher accuracy

- stats + MA:69.XX?
- KAKAO BRAIN: 73.33?

### Winner Talk
- Hypergraph Attention Networks for GQA

- alignment of information level between modalities
 - symbolic and structural representations
 - scene graphs + dependency trees
- higner-order correlation between modalities
 - structural semantic matching
 - random-walks on graphs
 
 **Steps**
 - Step1: Construct two symbolic graphs: 
  - scene graphs for images
  - word tokens with dependency trees
  
 - Step2: Transition Probability; Construct HG
 
 - Step3: Attention Matrix; Final Representation
 
 **Competetion**
 - ensemble 15 models
 
---
## TextVQA Challenge
### Task Overview
#### Overview
- Goal: navigation; blind users; VQA models can't read

- Data Issues:
 - TextVQA: number of questions with answers in ocr:39%
 
- Model Issues:
 - current VQA models cannot read
 
 - Challenges: open-ended vocabulary; multi-word predictions; text detection; text extraction; Reasoning; world knowledge
 

#### Baseline
- LoRRA: Look, Read, Reason and Answer

#### Challenge
- Starter Code: pythia
- 8 teams
- runnerup:30.54
- Winnders:31.44

### runnerup talk
- Staring at the data
 - including n-grams
 - spell correcting OCR
 - Ensembling and Vocabulary Expansion
 - Shuffling OCR
 - Data Augmentation 
 
- How you fuse feature matters

- Additional interesting sub-problems
 - learning to read time
 - learning to read logs
 
### Winner talk
- 3 parts: feature encoding part; attention and feature fusion part; answer embedding part


---
## Invited Talk4 Karl Moritz Hermann
- Grounded Language Learning

- Grounding Language: learning language through association with other sensory-motro experiences
- surface form similarity != semantic similarity

### How to deal with it?
- A dataset of learning spatial relations: CG; Similiar to scene representation netwokr
- Encoding tet + viewpoint to learn spatial representations
- Scene Rendering from natural language; Use the Scene Representation Network

### Language Grounding in Street View
- Street View + Reinforcement Learning
- real world; diversity imagery
- initial task: following the instructions from google maps
- Visual-Language Navigation in real environment dataset

- Encoding Spatial Relations from natural language
- Learning to Navigate in Cities without a Map
- Learning to Follow Directions in Street View
---
## Invited Talk5 Layla El Asri
- State Aliasing in Dialog Modeling with RNNs
- State Aliasing in RL: Experiments with RNNs
 - A study of state aliasing in structured prediction with RNNs
 - Building a representation based on actions might not allow to recover the optimal policy
- State Aliasing in RL: Experiments with compound actions
- State Aliasing in RL: Setting up a dialog task
- State Aliasing in RL: Model for dialog task
- State Aliasing in RL: Experiments on the dialog task

- Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat

*GuessWhat*
- train model with maximum-likelihood objective
- finetune with policy-gradient (both sentence and dialog level)

*???*
- state Aliasing in RL: Characterizing the problem


---
## Visual Dialog Challenge

### Overview
- Why VD:
 - natural language instructions for robots
 - aid visually impaired users
 - aid situationally impaired users
 
 - VisDial1.0:
  - 130 M coco images
  
 - Evaluation:
  - mean rank, recall@k
  - For multiple correct answer, dense annotations on v1.0 test, answer with relevant scores
  - NDCG: normalized discounted cumulative gain (swap rank between answers with the same references do not change the results)

- Models:
 - decoders: generative; discriminative
 
### Winner
- MReaL- BDAI 74.57 NDCG 
- best MRR team have relatively biased answer distribution

### Results Analysis
- discriminative vs. generative
 - generative models tend to create longer answers
 
### Winner Talk
- Learning to Answer: Fine-tuning with Generalized Cross Entropy for visual dialog challenge

#### Task
- Visual Dialog Challenge 2019 provides extra dense annotation
- 


####
- Structure the dialog history
- Encoder: RvA+ Recursive Visual Attention
- Decoder: discriminative

#### Tips
- Remove history from joint embedding
 - avoid picking long answers; avoid overfitting

- Multi-head
- Ensemble
- RvA+
- Complicate features
 - bbox
 
- Two-Stage Traning
 - stage 1: learn to reason
 - stage 2: learn to answer

#### Analysis 
- what's next
 - utilize history for visual reasoning
 - reduce the inconsistency of annotations

---
## Invited Talk6 Sanja Fidler
- Compositional Learning of Complex Task

- Multi-task Learning
 - tasks should be benifit from each other
 
- Learning in curriculum
 - learn gradually

- Most task can be break into small tasks


### Visual Reasoning by Prograssive Module Network (ICLR2019)
- use small modules to solve the bigger module
- terminal modules
- Compositional modules
 
 
 *PMN for Visual Reasoning*
 - Level 0: object recogniton; attribute recogniton
 - Level 1: Image Captioning; relationship detection
 - Level 2: Object Counting
 - Level 3: VQA
 - high interpretability
 
 ### Learning to Caption by Asking questions
 - Describes images with Humans in the Loop
- Use language to guide learning agent






---
## Invited Talk7 Yoav Artzi


---
## Future Direction



---
## Closing Remark


---
