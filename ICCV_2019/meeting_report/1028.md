# 3rd Workshop on Closing the Loop Between Vision and Language (CLVL)

## Indexing:
- [Opening Remarks](#Opening-Remarks)
- [Invited Talk-Sanja Fidler](#Invited-Talk-Sanja-Fidler)
- [Spotlight Presentation 1](#Spotlight-Presentation-1)
- [Invited Talk-Svetlana Lazabnik](#Invited-Talk-Svetlana-Lazabnik)
- [Invited Talk-Yejin Choi](#Invited-Talk-Yejin-Choi)
- [Spotlight Presentation 2](#Spotlight-Presentation-2)
- [Invited Talk-Gunhee Kim](#Invited-Talk-Gunhee-Kim)
- [VATEX Challenge](#VATEX-Challenge)
- [LSMDC Challenge](#LSMDC-Challenge)
- [Invited Talk-Devi Parikh and Jiasen Lu](#Invited-Talk-Devi-Parikh-and-Jiasen-Lu)

---
## Invited Talk-Sanja Fidler
Learning Representations of Vision and Language
- Multimodal Learning: more than one modality as input
- Difficulty: different statistical properties
- Bilinear model is more expressive
- CB for uni-modality
- MCB
- Low-rank bilinear (ICLR 2017)
- MLB, MUTAN, MFB
- Scaled dot-product
- Multi-head attention
- Co-attention
- Bilinear attention networks: joint attention based on low-rank 
- Deep modular co-attention: transformer-based approaches
- Low-rank factorization!

http://wityworks.com/

---
## Spotlight Presentation 1
- Are we asking the right questions in MovieQA? 
Previous works did not obtain high accuracy than QA only.
Wikiword embeddingmodel: SOTA on 4/5 on movieQA categories.

- Referring expression
A dataset with depth, images and especially for the spatial relationship
Applications: generation of expression, understanding

- Evalutating text-to-image
Evalutating is difficult in image captioning
Is image retieval good? (No)
BISON dataset for binary picking: good for evaluation.
Can be used for both image captioning and image retrieval

- Visual storytelling
Visual storytelling does not equal to image captioning.
Each narrative story should have one sequence of anchor words
SOTA on visual storytelling dataset.

- Why does a vqa have different answers
crowd answers matters
Novel problem, new dataset, algorithm to learn why different answers arise.
Connection with VizWiz dataset.

- Diversity-accuracy tradeoff in image captioing
ALLSPICE
a new metric to evaluate image cpations: accuracy and diversity.
Tree structure.
Reinforcement learning hurts diversity.
Top-k and nucleus are marginally better.

- nocaps: novel object captioning at scale
SOTA on COCO dataset.
Biased coco image captioning dataset.
Training: coco, open images
Still a much harder benchmark.

- Image captioning with versy scarce supervised data: adversarial semi-supervised
Leveraging unpaired caption data.
Used GAN to decide the true caption

- 
faster-r-cnn trained to predict object+attributes in visual genome
decoupled bbox, region ultra
---
## Invited Talk-Svetlana Lazabnik
- A Critical Look at Visual Grounding
Grounding is fundamental 
---
## Invited Talk-Yejin Choi
- Can’t Close the Loop without Commonsense Models


---
## Spotlight Presentation 2


---
## Invited Talk-Gunhee Kim
- Audio captioning and knowledge-grounded conversation


---
## VATEX Challenge


--- 
## LSMDC Challenge




---
## Invited Talk-Devi Parikh and Jiasen Lu
- V&L --> V ∪ L: Breaking away from task- and dataset-specific vision+language 


---
