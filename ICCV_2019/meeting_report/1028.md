# 3rd Workshop on Closing the Loop Between Vision and Language (CLVL)

## Indexing:
- [Opening Remarks](#Opening-Remarks)
- [Invited Talk-Sanja Fidler](#Invited-Talk-Sanja-Fidler)
- [Spotlight Presentation 1](#Spotlight-Presentation-1)
- [Invited Talk-Svetlana Lazabnik](#Invited-Talk-Svetlana-Lazabnik)
- [Invited Talk-Yejin Choi](#Invited-Talk-Yejin-Choi)
- [Spotlight Presentation 2](#Spotlight-Presentation-2)
- [Invited Talk-Gunhee Kim](#Invited-Talk-Gunhee-Kim)
- [VATEX Challenge](#VATEX-Challenge)
- [LSMDC Challenge](#LSMDC-Challenge)
- [Invited Talk-Devi Parikh and Jiasen Lu](#Invited-Talk-Devi-Parikh-and-Jiasen-Lu)

---
## Invited Talk-Sanja Fidler
Learning Representations of Vision and Language
- Multimodal Learning: more than one modality as input
- Difficulty: different statistical properties
- Bilinear model is more expressive
- CB for uni-modality
- MCB
- Low-rank bilinear (ICLR 2017)
- MLB, MUTAN, MFB
- Scaled dot-product
- Multi-head attention
- Co-attention
- Bilinear attention networks: joint attention based on low-rank 
- Deep modular co-attention: transformer-based approaches
- Low-rank factorization!

http://wityworks.com/

---
## Spotlight Presentation 1


---
## Invited Talk-Svetlana Lazabnik
- A Critical Look at Visual Grounding

---
## Invited Talk-Yejin Choi
- Can’t Close the Loop without Commonsense Models


---
## Spotlight Presentation 2


---
## Invited Talk-Gunhee Kim
- Audio captioning and knowledge-grounded conversation


---
## VATEX Challenge


--- 
## LSMDC Challenge




---
## Invited Talk-Devi Parikh and Jiasen Lu
- V&L --> V ∪ L: Breaking away from task- and dataset-specific vision+language 


---
