# 3rd Workshop on Closing the Loop Between Vision and Language (CLVL)

## Indexing:
- [Opening Remarks](#Opening-Remarks)
- [Invited Talk-Sanja Fidler](#Invited-Talk-Sanja-Fidler)
- [Spotlight Presentation 1](#Spotlight-Presentation-1)
- [Invited Talk-Svetlana Lazabnik](#Invited-Talk-Svetlana-Lazabnik)
- [Invited Talk-Yejin Choi](#Invited-Talk-Yejin-Choi)
- [Spotlight Presentation 2](#Spotlight-Presentation-2)
- [Invited Talk-Gunhee Kim](#Invited-Talk-Gunhee-Kim)
- [VATEX Challenge](#VATEX-Challenge)
- [LSMDC Challenge](#LSMDC-Challenge)
- [Invited Talk-Devi Parikh and Jiasen Lu](#Invited-Talk-Devi-Parikh-and-Jiasen-Lu)

---
## Invited Talk-Sanja Fidler
Learning Representations of Vision and Language
- Multimodal Learning: more than one modality as input
- Difficulty: different statistical properties
- Bilinear model is more expressive
- CB for uni-modality
- MCB
- Low-rank bilinear (ICLR 2017)
- MLB, MUTAN, MFB
- Scaled dot-product
- Multi-head attention
- Co-attention
- Bilinear attention networks: joint attention based on low-rank 
- Deep modular co-attention: transformer-based approaches
- Low-rank factorization!

http://wityworks.com/

---
## Spotlight Presentation 1
- Are we asking the right questions in MovieQA? 
Previous works did not obtain high accuracy than QA only.
Wikiword embeddingmodel: SOTA on 4/5 on movieQA categories.

- Referring expression
A dataset with depth, images and especially for the spatial relationship
Applications: generation of expression, understanding

- Evalutating text-to-image
Evalutating is difficult in image captioning
Is image retieval good? (No)
BISON dataset for binary picking: good for evaluation.
Can be used for both image captioning and image retrieval

- Visual storytelling
Visual storytelling does not equal to image captioning.
Each narrative story should have one sequence of anchor words
SOTA on visual storytelling dataset.

- Why does a vqa have different answers
crowd answers matters
Novel problem, new dataset, algorithm to learn why different answers arise.
Connection with VizWiz dataset.

- Diversity-accuracy tradeoff in image captioing
ALLSPICE
a new metric to evaluate image cpations: accuracy and diversity.
Tree structure.
Reinforcement learning hurts diversity.
Top-k and nucleus are marginally better.

- nocaps: novel object captioning at scale
SOTA on COCO dataset.
Biased coco image captioning dataset.
Training: coco, open images
Still a much harder benchmark.

- Image captioning with versy scarce supervised data: adversarial semi-supervised
Leveraging unpaired caption data.
Used GAN to decide the true caption

- 
faster-r-cnn trained to predict object+attributes in visual genome
decoupled bbox, region ultra
---
## Invited Talk-Svetlana Lazabnik
- A Critical Look at Visual Grounding
Grounding is fundamental 
---
## Invited Talk-Yejin Choi
- Can’t Close the Loop without Commonsense Models
Visual Commonsense dataset (Answer Justification)
Big gap between human and SOTA methods
Definition of Common Sense
ATomic: cases; effects;
ACL 2019: COMeT

---
## Spotlight Presentation 2
- Visual Question Generation
- Learning Semantic Sentence Embeddings using Pair-wise Discriminator
Sentence have different forms.
- Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning
Sequence CVAEs
Adding a kind of intention model
- Reinforcing an Image Caption Generator using Off-line Human Feedback (Sequence CVAEs)
Improve image captioning model using caption rating dataset
- Use What You Have: Video retrieval using representations from collaborative experts
Collaborative experts (a lot of different expert methods)
We need collaborative experts and also we need all kinds of experts

- ICDAR 2019 Competition on Scene Text Visual Question Answering (Main conference)
A dataset
Scene text visual question answering
A new evaluation metric 

- Recognizing and Characterizing Natural Language Descriptions of Visually Complex Images
Visual complexity, Images with multi-objects.
Human evaluation: answering time 

- Adversarial Learning of Semantic Relevance in Text to Image Synthesis
Diversity in text to image synthesis


---
## Invited Talk Gunhee Kim
- Audio captioning and knowledge-grounded conversation
First audio captioning dataset in the wild.
Video captioning is different from video captioning
50K 10sec audio-caption pairs
8 super-categories (72 categories)
Annotation inteface: word label pairs from audioset as hints
LSTM with topdown attention and aligned semantic attention  (temporal & semantic attention)
Do the captions map the audio uniquely?
What can we do to improve the caption quality? (VGGish, AlighedAtt, TopDown attention)

- Sequential Latent Knowledge Selection for Knowledge-grounded Dialog
s2s model tend to produce non-specific response
Incorporating knowledge to generate informative response
Wizard of Wikipedia (WoW) dataset
Two speakers: Wizard and appretice, pick topic and speak first
Knowledge selection and Utterance Prediction
Tracking selection along dialogue can reduce scope of knowledge selection

---
## VATEX Challenge
- [paper link](https://arxiv.org/pdf/1904.03493.pdf)
41.3K unique video clips, 600 human activities
Longer sentences than MSR-VTT
0 duplicated sent rate
task 1: multilingual video captioning
task 2: Video-guided machine translation 
Metrics: addition of four popular metrics

- Multi-modal information fusion and multi-stage training strategy for video captioning
Encoder-decoder
make uses of attributes, attention, region, etc.

Multi-modal Information (appeaance, motion,..., maybe audio)

Multi-stage training strategy (gumble softmax sampling to replace scheduled, self-critical based reinforcement learning)

### Runner-up Presentation
multi-level aspects (Glo bal multimodal features;temporal encoding branch; spatial encoding brach)
self-critic RL
VSE

--- 
## LSMDC Challenge
- [Challenge page](https://sites.google.com/site/describingmovies/home?authuser=0)
200 videos
Still a large gap to human performance
Task 1: multi-clips multi-sentence description generation
Task 2: fill in the characters 

---
## Invited Talk-Devi Parikh and Jiasen Lu
- V&L --> V ∪ L: Breaking away from task- and dataset-specific vision+language 



---
